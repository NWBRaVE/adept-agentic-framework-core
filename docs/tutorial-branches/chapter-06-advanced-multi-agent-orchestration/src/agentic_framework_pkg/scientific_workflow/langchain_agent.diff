diff --git a/docs/tutorial-branches/chapter-06-advanced-multi-agent-orchestration/src/agentic_framework_pkg/scientific_workflow/langchain_agent.py b/docs/tutorial-branches/chapter-06-advanced-multi-agent-orchestration/src/agentic_framework_pkg/scientific_workflow/langchain_agent.py
index e5d8fce..2363638 100644
--- a/docs/tutorial-branches/chapter-06-advanced-multi-agent-orchestration/src/agentic_framework_pkg/scientific_workflow/langchain_agent.py
+++ b/docs/tutorial-branches/chapter-06-advanced-multi-agent-orchestration/src/agentic_framework_pkg/scientific_workflow/langchain_agent.py
@@ -38,9 +38,9 @@ from .mcp_langchain_tools import (
 )
 
 # LangGraph imports
-from langgraph.prebuilt import create_react_agent # Use create_react_agent
-# ToolMessage is still useful for understanding message types if needed, but not directly for graph construction here
-# TypedDict, Annotated, Sequence, operator might be removed if AgentState class is removed/simplified
+from langchain_core.agents import AgentFinish
+from langgraph.graph import StateGraph, END
+from langgraph.prebuilt import ToolNode
 
 from ..logger_config import get_logger # Use centralized logger
 from ..core.llm_agnostic_layer import LLMAgnosticClient # Import the agnostic client
@@ -51,17 +51,89 @@ LANGCHAIN_LLM_MODEL = os.getenv("LANGCHAIN_LLM_MODEL", "gpt-3.5-turbo") # Or "gp
 
 # Define the state for LangGraph
 class AgentState(TypedDict):
+    """Agent state
+     # If you need to pass other specific data through the graph, add it here.
+     # For example: mcp_session_id if it needs to be explicitly in the state
+     # mcp_session_id: Optional[str] # create_react_agent handles state internally; mcp_session_id is in prompt and tool wrappers
+     # The default AgentState for create_react_agent includes 'is_last_step' and 'remaining_steps'
+     # We can let create_react_agent use its default state by not passing state_schema,
+     # or align this definition if we choose to pass it. For simplicity, let's remove this custom AgentState for now.
+     # Note: To support a more robust plot generation for both streamlit and openwebui,
+             the new split-stream strategy requires custom logic to create the langgraph agent (instead of the create_react_agent utility method).
+             The custom logic delineates the full/raw tool output vs what should be shared with the LLM's context. Previously, the ToolMessage
+             would be populated with not only the plot_url but the full base64 encoded content, which would fill up the context window especially
+             for non-Gemini models (e.g., ollama/chatgpt has at most 200K context window).
+    """
     messages: Annotated[Sequence[BaseMessage], operator.add]
-    # If you need to pass other specific data through the graph, add it here.
-    # For example: mcp_session_id if it needs to be explicitly in the state
-    # mcp_session_id: Optional[str] # create_react_agent handles state internally; mcp_session_id is in prompt and tool wrappers
-    # The default AgentState for create_react_agent includes 'is_last_step' and 'remaining_steps'
-    # We can let create_react_agent use its default state by not passing state_schema,
-    # or align this definition if we choose to pass it. For simplicity, let's remove this custom AgentState for now.
+    chat_history: List[Dict[str, str]]
+    full_tool_outputs: Annotated[List[Dict[str, Any]], operator.add]
 
 # This is a conceptual setup. A real agent would need more robust memory, error handling, etc.
 
 class ScientificWorkflowAgent:
+    """Main scientific workflow agent
+
+    Some design discussions on workflow graph construction:
+
+    In short, our custom StateGraph is more robust for our specific needs because it gives us complete control, whereas create_react_agent is a high-level utility that
+    prioritizes ease of use over flexibility.
+
+    Here’s a detailed breakdown of the trade-offs:
+
+    create_react_agent (The Utility Function)
+
+    This is a pre-packaged solution provided by LangGraph to quickly create a standard ReAct (Reasoning + Acting) agent.
+
+    * Robustness in Simplicity:
+       * High Reliability: It's built and maintained by the LangChain team and is optimized for the most common agent workflow. It's stable, well-tested, and less likely to
+         have subtle bugs in its core looping logic.
+       * Ease of Use: It requires very little boilerplate code. You provide a model and tools, and it handles the entire graph construction for you.
+
+    * Where its "Robustness" Ends (The Limitations):
+       * Inflexibility: It is a "black box." You cannot easily modify the internal workflow. We could not, for example, intercept the tool output to create our
+         "split-stream" data flow.
+       * Opacity: Because the internal logic is hidden, it can be harder to debug when things go wrong in non-standard ways. You don't have direct control over the nodes or
+         the edges connecting them.
+
+    Custom StateGraph (Our Current Approach)
+
+    This approach involves manually defining every node (e.g., agent, action) and every edge (the logic that connects them).
+    
+    * Robustness in Control and Flexibility:
+       * Maximum Control: This is its greatest strength. We have complete authority over the data flow. We were able to create a _custom_tool_node to intercept the tool's
+         output, sanitize it for the LLM, and store the full version in our state. This is the kind of advanced, granular control that create_react_agent does not permit.
+       * Transparency: The logic is explicit in our code. We can see exactly how the agent state is passed and modified at every step, which makes debugging complex
+         interactions much easier.
+       * Extensibility: It is far easier to add new, complex behaviors. If we wanted to add a human-in-the-loop approval step, a new conditional branch, or a
+         "self-correction" loop, we could do so by adding new nodes and edges to our graph.
+
+    * Where its "Robustness" Requires Care:
+       * Increased Complexity: It is more verbose and requires a deeper understanding of LangGraph's state management.
+       * Maintenance Overhead: We are now responsible for maintaining this workflow logic. If LangGraph introduces breaking changes to StateGraph, we might need to update
+         our code, whereas create_react_agent would handle that internally.
+
+    Comparison Table
+
+
+    ┌──────────────┬──────────────────┬───────────────────┐
+    │ Aspect       │ create_react_agent │ Custom StateGraph │
+    ├──────────────┼──────────────────┼───────────────────┤
+    │ Control      │ Low              │ High              │
+    │ Flexibility  │ Low              │ High              │
+    │ Simplicity   │ High             │ Low               │
+    │ Transparency │ Low              │ High              │
+    │ Maintenance  │ Low              │ Medium            │
+    └──────────────┴──────────────────┴───────────────────┘
+
+    Verdict
+
+    The create_react_agent function is robust for standard agentic workflows. However, our requirement to manage the LLM's context by sanitizing tool outputs pushed us
+    beyond what it could offer.
+
+    Our custom StateGraph is more robust for our advanced use case precisely because it is flexible enough to accommodate the "split-stream" strategy. We traded the
+    simplicity of the utility function for the power and control of a custom-built graph, which was necessary to solve the problem at hand.
+    
+    """
     def __init__(self, mcp_session_id: Optional[str] = None):
         self.mcp_session_id = mcp_session_id # Crucial for stateful MCP tool calls
         self.llm_agnostic_client = LLMAgnosticClient()
@@ -138,7 +210,7 @@ class ScientificWorkflowAgent:
             "You can also create and manage a team of expert AI agents (multi-agent team) for complex tasks. This is a complete lifecycle process. You can list all active sessions using 'ListActiveMultiAgentSessions' to see which teams are available. "
             "First, use 'CreateMultiAgentSession' to build a team with specific roles (e.g., chemist, data scientist). You will get a session ID back. "
             "Second, use 'GeneratePlanForMultiAgentTask' with that session ID to create a plan for the team's supervisor. When this tool returns a plan, you MUST display the entire plan to the user and ask for their approval before proceeding. "
-            "Third, if the user provides an edited version of the plan, you must use the 'UpdatePendingPlan' tool to save the changes. "
+            "Third, if the user provides an edited version of the plan, you must use the 'UpdatePendingPlan' tool to save the changes. If the user simply approves the plan (e.g., by saying 'yes' or 'looks good' or 'I approve of the plan'), you MUST NOT generate the plan again. Instead, you must immediately call the 'ExecuteApprovedPlan' tool to start the execution. "
             "Fourth, once the user approves the plan (either the original or the edited version), you must call 'ExecuteApprovedPlan' with the same session ID to run the plan and get the final report. "
             "Finally, after the task is complete and the user is satisfied, you should use 'TerminateMultiAgentSession' to clean up the session resources. "
             "If there are processed files avaiable, you can use the QueryProcessedCSVData tool to query processed files. "
@@ -155,20 +227,89 @@ class ScientificWorkflowAgent:
             # example_json_str_for_prompt is already formatted if needed here, but it's part of the template string
         )
 
-        # LangGraph setup using create_react_agent
-        # The `create_react_agent` function handles the creation of the agent,
-        # tool node, and the graph logic (looping between agent and tools).
-        self.graph = create_react_agent(
-            model=self.llm,
-            tools=self.tools,
-            prompt=SystemMessage(content=self.formatted_system_prompt), # Pass the system prompt as a SystemMessage
-            # state_schema=AgentState, # Can be omitted to use create_react_agent's default AgentState
-            # checkpointer=None, # Add if you need persistence
-            debug=True # Enable LangGraph debug logging
+        # --- Graph Definition ---
+        # Custom to support split-stream of base64 encoded tool output
+        workflow = StateGraph(AgentState)
+
+        # Define the nodes
+        workflow.add_node("agent", self.call_model)
+        workflow.add_node("action", self._custom_tool_node)
+
+        # Set the entrypoint
+        workflow.set_entry_point("agent")
+
+        # Add conditional edges
+        workflow.add_conditional_edges(
+            "agent", self.should_continue, {"continue": "action", "end": END}
         )
-        logger.info("LangGraph workflow compiled using create_react_agent.")
+        workflow.add_edge("action", "agent")
+
+        self.runnable = workflow.compile()
+        logger.info(f"LangGraph agent compiled successfully for session {self.mcp_session_id}.")
+
+    def should_continue(self, state: AgentState) -> str:
+        last_message = state["messages"][-1]
+        if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
+            return "end"
+        return "continue"
+
+    def call_model(self, state: AgentState) -> dict:
+        messages = state["messages"]
+        response = self.llm.invoke(messages)
+        return {"messages": [response]}
+
+    def _sanitize_tool_output(self, output: Dict[str, Any]) -> Dict[str, Any]:
+        """Sanitize only 'plot' output; this is being very esoteric now.
+
+        The sanitization logic is surgical and only activates under a very specific condition. It is handled by the _sanitize_tool_output method, which contains this check:
+
+        1 if isinstance(output, dict) and "plots" in output and isinstance(output["plots"], list):
+        2     # ... sanitization logic runs only inside this block ...
+
+        This code acts as a guard:
+
+        1. It first checks if the tool's output is a dictionary.
+        2. It then checks if that dictionary explicitly contains a key named `"plots"`.
+
+        Tool outputs from QueryUniProt or the web search tools return JSON that includes data like summaries, entries, or sequences, but they do not contain a "plots" key.
 
-    # Manual graph node methods (_call_model_node_lg, _call_tool_node_lg, _should_continue_node_lg) are no longer needed.
+        Therefore, their output will fail the if condition, and the function will immediately return output without any modification. The full, original text data from those
+        tools will be passed to the LLM, exactly as it was before.
+
+        The sanitization is designed to be dormant unless it sees the specific signature of a plot-generating tool, ensuring it doesn't interfere with any other part of the
+        workflow.
+        
+        """
+        if isinstance(output, dict) and "plots" in output and isinstance(output["plots"], list):
+            sanitized_plots = []
+            for plot in output["plots"]:
+                if isinstance(plot, dict) and "content_base64" in plot:
+                    sanitized_plot = plot.copy()
+                    sanitized_plot["content_base64"] = f"<{sanitized_plot.get('format', 'image').upper()} data omitted, use plot_url>"
+                    sanitized_plots.append(sanitized_plot)
+            output["plots"] = sanitized_plots
+        return output
+
+    def _custom_tool_node(self, state: AgentState) -> dict:
+        tool_messages_for_llm = []
+        full_tool_outputs_for_state = []
+        last_message = state['messages'][-1]
+        
+        for tool_call in last_message.tool_calls:
+            tool_to_call = next((t for t in self.tools if t.name == tool_call["name"]), None)
+            if not tool_to_call:
+                raise ValueError(f"Tool '{tool_call['name']}' not found.")
+            
+            raw_output_str = tool_to_call.invoke(tool_call["args"])
+            raw_output_dict = json.loads(raw_output_str)
+            full_tool_outputs_for_state.append(raw_output_dict)
+
+            sanitized_output_dict = self._sanitize_tool_output(raw_output_dict)
+            sanitized_output_str = json.dumps(sanitized_output_dict)
+            
+            tool_messages_for_llm.append(ToolMessage(content=sanitized_output_str, tool_call_id=tool_call['id']))
+
+        return {"messages": tool_messages_for_llm, "full_tool_outputs": full_tool_outputs_for_state}
 
     # --- Main Agent Execution Method ---
     async def arun(self, user_input: str, chat_history: Optional[List[Dict[str, str]]] = None, callbacks: Optional[List[Union[BaseCallbackHandler, AsyncCallbackHandler]]] = None, recursion_limit: int = 25) -> Dict[str, Any]:
@@ -187,10 +328,14 @@ class ScientificWorkflowAgent:
                 elif msg.get("role") == "assistant":
                     current_chat_history.append(AIMessage(content=msg.get("content","")))
         
-        # `create_react_agent` graph expects input with a "messages" key.
-        # It manages its own state, including 'remaining_steps'.
-        initial_messages_for_graph = current_chat_history + [HumanMessage(content=user_input)]
-        graph_input = {"messages": initial_messages_for_graph}
+        # The input to the custom graph needs to match AgentState.
+        # We prepend the system prompt here, as it's not part of the compiled graph itself.
+        initial_messages = [SystemMessage(content=self.formatted_system_prompt)] + current_chat_history + [HumanMessage(content=user_input)]
+        graph_input = {
+            "messages": initial_messages,
+            "chat_history": chat_history or [],
+            "full_tool_outputs": []
+        }
         
         # Pass callbacks and recursion limit to the graph invocation
         langchain_config = {
@@ -198,10 +343,10 @@ class ScientificWorkflowAgent:
             "recursion_limit": recursion_limit
         }
 
-        logger.info(f"Running create_react_agent graph for session {self.mcp_session_id} with input: {user_input}")
+        logger.info(f"Running custom LangGraph for session {self.mcp_session_id} with input: {user_input}")
         try:
-            # `create_react_agent` returns the final state of the graph.
-            final_state_result = await self.graph.ainvoke(graph_input, config=langchain_config)
+            # The custom compiled graph is now named 'self.runnable'
+            final_state_result = await self.runnable.ainvoke(graph_input, config=langchain_config)
             
             if final_state_result and final_state_result.get('messages'):
                 last_message_in_graph = final_state_result['messages'][-1]
