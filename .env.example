# LLM API Keys and Configuration

# --- Generic Fallback LLM Configuration ---
# These are used if purpose-specific configurations or models are not set.
DEFAULT_LLM_MODEL="o3-mini" # Example: "gpt-3.5-turbo", "azure/your-generic-deployment", "ollama/mistral"

# Generic Azure (used if no <PURPOSE>_AZURE_API_KEY/BASE/VERSION is set for a given purpose)
#OPENAI_API_KEY="sk-your_openai_api_key"
#ANTHROPIC_API_KEY="sk-ant-your_anthropic_api_key"
AZURE_API_KEY="zzz" # TODO: Add your own. Required for Azure models
#AZURE_API_BASE="https://aqe-ldrd-openai.openai.azure.com/" # Required, e.g., https://YOUR_RESOURCE_NAME.openai.azure.com/
AZURE_API_BASE="https://aqe-ldrd-openai.openai.azure.com/openai/deployments/o3-mini/chat/completions?api-version=2025-01-01-preview" # Required
AZURE_API_VERSION="2025-01-01-preview" # Required, e.g., 2023-07-01-preview or 2024-02-15-preview

# Model for Langchain components (e.g., ScientificWorkflowAgent)
LANGCHAIN_LLM_MODEL="o3-mini" # Set to your Azure deployment name if using Azure, or a general model name.

# Generic OpenAI (used if no <PURPOSE>_OPENAI_API_KEY is set for a given purpose, and Azure is not configured)
# OPENAI_API_KEY="sk-your_generic_openai_api_key"

# Google Vertex AI Configuration
# VERTEXAI_PROJECT="your_gcp_project_id" # Required
# VERTEXAI_LOCATION="your_gcp_region" # Required, e.g., us-central1

# Amazon Bedrock Configuration (Requires AWS credentials configured via env vars or AWS config)
# AWS_ACCESS_KEY_ID="your_aws_access_key_id" # Optional if using shared credentials/config
# AWS_SECRET_ACCESS_KEY="your_aws_secret_access_key" # Optional if using shared credentials/config
# AWS_REGION_NAME="your_aws_region" # Required, e.g., us-east-1

# Generic Ollama (used if no <PURPOSE>_OLLAMA_API_BASE is set for a given purpose, and model starts with "ollama/")
OLLAMA_API_BASE="http://localhost:11434" # If using local Ollama, adjust if running in Docker

# Groq (XAI) Configuration
# GROQ_API_KEY="pk-your_groq_api_key" # Required

# --- Streamlit LLM Configuration ---
STREAMLIT_DEFAULT_MODEL="o3-mini" # Example: "gpt-4-turbo-preview", "azure/streamlit-deployment"
# STREAMLIT_AZURE_API_KEY="..."
# STREAMLIT_AZURE_API_BASE="..."
# STREAMLIT_AZURE_API_VERSION="..."
# STREAMLIT_OPENAI_API_KEY="..."
# STREAMLIT_OLLAMA_API_BASE="..."

# --- Embedding LLM Configuration ---
EMBEDDING_DEFAULT_MODEL="text-embedding-3-large" # Example: "text-embedding-ada-002", "azure/embedding-deployment", "ollama/nomic-embed-text"
EMBEDDING_AZURE_API_KEY="zzz" # TODO: Add your own. Required for Azure embedding models
# EMBEDDING_AZURE_API_KEY="..."
# EMBEDDING_AZURE_API_BASE="..." # e.g. https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_EMBEDDING_DEPLOYMENT_NAME/embeddings?api-version=YOUR_API_VERSION
EMBEDDING_AZURE_API_BASE="https://aqe-ldrd-openai.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2023-05-15"
# # Ensure this API version is compatible with text-embedding-3-large, e.g., 2024-02-15-preview or 2024-03-01-preview
EMBEDDING_AZURE_API_VERSION="2023-05-15" # Or another recent, compatible version
# EMBEDDING_AZURE_API_VERSION="..."
# EMBEDDING_OPENAI_API_KEY="..."
# EMBEDDING_OLLAMA_API_BASE="..."

# --- RAG LLM Configuration ---
RAG_DEFAULT_MODEL="o3-mini" # Example: "gpt-3.5-turbo", "azure/rag-deployment"
# RAG_AZURE_API_KEY="..."
# RAG_AZURE_API_BASE="..."
# RAG_AZURE_API_VERSION="..."
# RAG_OPENAI_API_KEY="..."
# RAG_OLLAMA_API_BASE="..."

# NVIDIA API Configuration (for multi-modal document analysis)
#RAG_NVIDIA_API_KEY="nvapi-YOUR_API_KEY_HERE"
# NVIDIA_API_BASE_URL="https://integrate.api.nvidia.com/v1" # Optional: Default is usually fine
# NVIDIA_MULTI_MODAL_MODEL_NAME="nvidia/llama-3.1-nemotron-nano-vl-8b-v1" # Optional: Default model for vision
NVIDIA_MODEL="nvidia/llama-3.1-nemotron-nano-vl-8b-v1" # Example: "nvidia/rag-llama2-7b", "nvidia/rag-mistral-7b"
NVIDIA_API_KEY="nvapi-zzzz" # TODO: Add your own.  If using NVIDIA RAG, set your API key here
NVIDIA_API_BASE_URL="https://integrate.api.nvidia.com/v1" # If using NVIDIA RAG, set the API base URL here
# RAG_NVIDIA_API_VERSION="2024-01-01" # If using NVIDIA RAG, set the API version here

# MCP Server Configuration
MCP_SERVER_HOST="0.0.0.0"
MCP_SERVER_PORT="8080"
UVICORN_WORKERS="2" # For SQLite, start with 1 worker

# Streamlit Configuration
STREAMLIT_SERVER_PORT="8501"
STREAMLIT_SERVER_ADDRESS="0.0.0.0"

# --- Service URLs ---
# These are used by services to communicate with each other.
# The values below are for `docker compose`. For local runs outside Docker, change to `localhost`.
# The Helm chart overrides these automatically for Kubernetes deployments.
DEFAULT_MCP_SERVER_URL="http://mcp_server:8080/mcp"
HPC_MCP_SERVER_URL="http://hpc_mcp_server:8081/mcp"
SANDBOX_MCP_SERVER_URL="http://sandbox_mcp_server:8082/mcp"

# --- HPC MCP Server Configuration ---
# Host for the HPC MCP server to listen on
HPC_MCP_SERVER_HOST=0.0.0.0
# Port for the HPC MCP server
HPC_MCP_SERVER_PORT=8081
# Number of Uvicorn workers for the HPC MCP server
HPC_UVICORN_WORKERS=1

# -- ChromaDB Configuration --
# Path for ChromaDB persistent storage.
# If commented out or not set, state_manager.py will use a default (e.g., "./chroma_data").
# For in-memory ChromaDB (ephemeral, data lost on restart), use ":memory:".
CHROMA_DB_PATH="./data/persistent_chroma_db"
# CHROMA_DB_PATH=":memory:"

# Database Configuration (deprecated, use ChromaDB))
DATABASE_URL=sqlite+aiosqlite:///./data/mcp_state.db

# File Storage Configuration
SHARED_UPLOAD_DIR="./data/uploaded_files" # Local file storage path for uploaded files

# Logging Configuration
LOGGING_LEVEL="DEBUG" # Set to DEBUG for more verbose logging. Set to INFO or WARNING for less verbose logging.

# --- GitLab Integration Configuration --- TODO: Abstract these into future connector classes
GITLAB_API_TOKEN="glpat-zzzz" # TODO: Add your own. Required for GitLab integration
GITLAB_INSTANCE_URL="https://tanuki.pnnl.gov" # Optional: Specify if you want to target a specific GitLab instance
# GITLAB_PROJECT_ID="your_gitlab_project_id" # Optional: Specify if you want to target a specific project
# GITLAB_PROJECT_NAME="your_gitlab_project_name" # Optional: Specify if you want to target a specific project by name

# --- Github Integration Configuration ---
# GitHub Access Token (for GitXRay to avoid rate limiting)
GH_ACCESS_TOKEN_GITXRAY="ghp_zzzz" # TODO: Add your own. Required for GitXRay=""

# --- Web Search Configuration ---
# Web search API keys and configurations for various providers.
# These are used by the WebSearchAgent and other components that require web search capabilities.
BRAVE_SEARCH_API_KEY="YOUR_BRAVE_API_KEY_HERE" # TODO: Add your own. Required for Brave Search
BRAVE_SEARCH_API_BASE="https://api.brave.com/search/v1" # Base URL for Brave Search API
GOOGLE_SEARCH_API_KEY="YOUR_GOOGLE_API_KEY_HERE" # TODO: Add your own. Required for Google Search
GOOGLE_SEARCH_CX="YOUR_GOOGLE_CX_HERE" # Custom Search Engine ID
GOOGLE_SEARCH_API_BASE="https://www.googleapis.com/customsearch/v1" # Base URL for Google Search API
BING_SEARCH_API_KEY="YOUR_BING_API_KEY_HERE" # TODO: Add your own. Required for Bing Search
BING_SEARCH_API_BASE="https://api.bing.microsoft.com/v7.0/search" # Base URL for Bing Search API
DUCKDUCKGO_SEARCH_API_BASE="https://api.duckduckgo.com" # Base URL for DuckDuckGo Search API
DUCKDUCKGO_SEARCH_API_KEY="YOUR_DUCKDUCKGO_API_KEY_HERE" # TODO: Add your own. Required for DuckDuckGo Search
